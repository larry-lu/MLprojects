{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classification of news articles\n",
    "\n",
    "Xiaoyu Lu\n",
    "\n",
    "In this assignment, we are going to use the Naive Bayes algorithm as a means to automatically classify news reports. In particular, we will build on the material that we presented in class and test the classifierâ€™s performance using different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "from glob import glob\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = getcwd()\n",
    "chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two functions we will use during this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def if_topic_in(topic, topic_list = topic_list):\n",
    "    \"\"\"function to determine if each entry belongs to our topic list\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    :param topic: list of many topics of one article\n",
    "    :param topic_list: list of pre-defined topics\n",
    "    \n",
    "    :returns: index of first element in the topic list that belongs to topic_list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ans = list(set(topic).intersection(topic_list))\n",
    "    except:\n",
    "        ans = \"\"\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanbody(text):\n",
    "    \"\"\"function to clean text by removing punctuations, and numbers\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    :param text: a string\n",
    "    \n",
    "    :returns: string with punctuations and numbers removed\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = text.replace('\\n',' ').lower().strip()\n",
    "    text = re.sub(\"[^a-z, A-Z]+\", \"\", text).split()\n",
    "    text = ' '.join(stemmer.stem(i) for i in text)\n",
    "    stemmed = ' '.join([word for word in text.split() if word not in stopwords_set])\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing the Reuters' dataset (Reuters-21578)\n",
    "\n",
    "For this project we are only interested in articles whose topic falls in the list of:\n",
    "\n",
    "``[\"money\", \"fx\", \"crude\", \"grain\", \"trade\", \"interest\", \"wheat\", \"ship\", \"corn\", \"oil\", \"dlr\", \"gas\", \"oilseed\", \"supply\", \"sugar\", \"gnp\", \"coffee\", \"veg\", \"gold\", \"soybean\", \"bop\", \"livestock\", \"cpi\"]``\n",
    "\n",
    "Some articles are of multiple topics separated by dashlines (e.g. \"money-fx\"). In this case, we will create duplicate articles with each article corresponding to one topic. This will inevitably reduce the accuracy of our output, but we can live with it for this project.\n",
    "\n",
    "We started with parsing through all the (`*.sgm`) files and create a list of all relevant articles as tuples (topic, body). The `BeautifulSoup` library is used for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_list = glob('reuters21578/*.sgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_list = [\"money\", \"fx\", \"crude\", \"grain\", \"trade\", \"interest\", \"wheat\", \n",
    "              \"ship\", \"corn\", \"oil\", \"dlr\", \"gas\", \"oilseed\", \"supply\", \"sugar\", \n",
    "              \"gnp\", \"coffee\", \"veg\", \"gold\", \"soybean\", \"bop\", \"livestock\", \"cpi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing reuters21578/reut2-000.sgm\n",
      "Finished parsing reuters21578/reut2-000.sgm\n",
      "Start parsing reuters21578/reut2-001.sgm\n",
      "Finished parsing reuters21578/reut2-001.sgm\n",
      "Start parsing reuters21578/reut2-002.sgm\n",
      "Finished parsing reuters21578/reut2-002.sgm\n",
      "Start parsing reuters21578/reut2-003.sgm\n",
      "Finished parsing reuters21578/reut2-003.sgm\n",
      "Start parsing reuters21578/reut2-004.sgm\n",
      "Finished parsing reuters21578/reut2-004.sgm\n",
      "Start parsing reuters21578/reut2-005.sgm\n",
      "Finished parsing reuters21578/reut2-005.sgm\n",
      "Start parsing reuters21578/reut2-006.sgm\n",
      "Finished parsing reuters21578/reut2-006.sgm\n",
      "Start parsing reuters21578/reut2-007.sgm\n",
      "Finished parsing reuters21578/reut2-007.sgm\n",
      "Start parsing reuters21578/reut2-008.sgm\n",
      "Finished parsing reuters21578/reut2-008.sgm\n",
      "Start parsing reuters21578/reut2-009.sgm\n",
      "Finished parsing reuters21578/reut2-009.sgm\n",
      "Start parsing reuters21578/reut2-010.sgm\n",
      "Finished parsing reuters21578/reut2-010.sgm\n",
      "Start parsing reuters21578/reut2-011.sgm\n",
      "Finished parsing reuters21578/reut2-011.sgm\n",
      "Start parsing reuters21578/reut2-012.sgm\n",
      "Finished parsing reuters21578/reut2-012.sgm\n",
      "Start parsing reuters21578/reut2-013.sgm\n",
      "Finished parsing reuters21578/reut2-013.sgm\n",
      "Start parsing reuters21578/reut2-014.sgm\n",
      "Finished parsing reuters21578/reut2-014.sgm\n",
      "Start parsing reuters21578/reut2-015.sgm\n",
      "Finished parsing reuters21578/reut2-015.sgm\n",
      "Start parsing reuters21578/reut2-016.sgm\n",
      "Finished parsing reuters21578/reut2-016.sgm\n",
      "Start parsing reuters21578/reut2-017.sgm\n",
      "Finished parsing reuters21578/reut2-017.sgm\n",
      "Start parsing reuters21578/reut2-018.sgm\n",
      "Finished parsing reuters21578/reut2-018.sgm\n",
      "Start parsing reuters21578/reut2-019.sgm\n",
      "Finished parsing reuters21578/reut2-019.sgm\n",
      "Start parsing reuters21578/reut2-020.sgm\n",
      "Finished parsing reuters21578/reut2-020.sgm\n",
      "Start parsing reuters21578/reut2-021.sgm\n",
      "Finished parsing reuters21578/reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "doi_list = list()\n",
    "for filename in f_list:\n",
    "    print('Start parsing {0}...'.format(filename))\n",
    "    file = open(filename, 'rb')\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    file.close()\n",
    "    for topic_raw in soup.find_all('topics'):\n",
    "        topic = topic_raw.get_text().split('-')\n",
    "        topic = if_topic_in(topic)\n",
    "        if len(topic) != 0:\n",
    "            body = topic_raw.find_next('body').get_text()\n",
    "            for t in topic:\n",
    "                tb_tup = (t, body)\n",
    "                doi_list.append(tb_tup)\n",
    "    print('Finished parsing {0}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(doi_list)\n",
    "data.columns = (['topic', 'body'])\n",
    "data['body'] = data['body'].apply(cleanbody)\n",
    "print('A total number of {0} items were retrieved. Articles with multiple classes are recorded multiple times.'.format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output is saved as a `pandas` DataFrame. A total number of 3625 items were retrieved. Articles with multiple topics are recorded multiple times.\n",
    "\n",
    "For convenience, we will save the data as a `.txt` file. The first 10 entries are also saved separately as a `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('training_test_data.txt')\n",
    "data.loc[0:10].to_csv('top10.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. TF-IDF transformation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn TFIDF processing time: 0.50861 s\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('training_test_data.txt')\n",
    "\n",
    "body_list = list(data['body'])\n",
    "start = time.clock()\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data['body'])\n",
    "print (\"sklearn TFIDF processing time: {0:.5f} s\".format(time.clock() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF transformation in `pyspark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import HashingTF, IDF, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "path = getcwd()\n",
    "chdir(path)\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"NewsClassification\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"training_test_data.txt\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_stem_udf = udf(stop_stem, ArrayType(StringType()))\n",
    "df = df.withColumn(\"body\", stop_stem_udf(\"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark TFIDF processing time: 0.01415 s\n"
     ]
    }
   ],
   "source": [
    "#following section transforms the text using TFIDF\n",
    "start = time.clock()\n",
    "hashingTF = HashingTF(inputCol=\"body\", outputCol=\"term_freq\")\n",
    "df = hashingTF.transform(df)\n",
    "idf = IDF(inputCol=\"term_freq\", outputCol=\"tfidf\", minDocFreq=5)\n",
    "idfModel = idf.fit(df)\n",
    "df = idfModel.transform(df)\n",
    "print (\"pyspark TFIDF processing time: {0:.5f} s\".format(time.clock() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the OneHotEncoder to convert the topics into discrete integers\n",
    "stringIndexer = StringIndexer(inputCol=\"topic\", outputCol=\"topicIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dict = dict()\n",
    "train_test_cv_split_params = {'50/40/10': [0.5, 0.4, 0.1],\n",
    "                               '60/30/10': [0.6, 0.3, 0.1], \n",
    "                               '70/20/10': [0.7, 0.2, 0.1]}\n",
    "\n",
    "for split_param in train_test_cv_split_params.keys(): #run the model for each train/test/cv split\n",
    "    for seed in np.arange(10): #run each model 10 times using different random seed\n",
    "        train,test,cv = indexed.select(\"tfidf\",\"topicIndex\").randomSplit(train_test_cv_split_params[split_param],seed=seed)\n",
    "\n",
    "        #Naive bayes\n",
    "        nb = NaiveBayes(featuresCol=\"tfidf\", labelCol=\"topicIndex\", predictionCol=\"NB_pred\",\n",
    "                        probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "        nbModel = nb.fit(train)\n",
    "        cv = nbModel.transform(cv)\n",
    "        total = cv.count()\n",
    "        correct = cv.where(test['topicIndex'] == cv['NB_pred']).count()\n",
    "        accuracy = correct/total\n",
    "        val_dict[(split_param, seed)] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combination of parameters that produced the highest accuracy (0.53): train/test/cv split ratio: 60/30/10, randomseed: 2\n"
     ]
    }
   ],
   "source": [
    "params = max(val_dict, key = val_dict.get)\n",
    "print(\"The combination of parameters that produced the highest accuracy ({0:.2f}): train/test/cv split ratio: {1}, randomseed: {2}\".format(max(val_dict.values()),params[0], params[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meancal(val_dict, split_param):\n",
    "    l = list()\n",
    "    for i in val_dict.keys():\n",
    "        if i[0] == split_param:\n",
    "            l.append(val_dict[i])\n",
    "    #print(l)\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy of the 30 models: 0.493\n",
      "The split condition 50/40/10 has a mean accuracy of 0.481\n",
      "The split condition 60/30/10 has a mean accuracy of 0.497\n",
      "The split condition 70/20/10 has a mean accuracy of 0.502\n"
     ]
    }
   ],
   "source": [
    "print('The mean accuracy of the 30 models: {0:.3f}'.format(np.mean(list(val_dict.values()))))\n",
    "\n",
    "for split_param in train_test_cv_split_params:\n",
    "    mean_accuracy = meancal(val_dict, split_param)\n",
    "    print('The split condition {0} has a mean accuracy of {1:.3f}'.format(split_param, mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the accuracy of the model increases as we have a higher proportion of training data. However, in our case the highest performing model has a `60/30/10` split instead of `70/20/10`, suggesting that our model is sensitive to the random split conditions, and we should run the model on each split condition multiple times to make sure our evaluation is not biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
