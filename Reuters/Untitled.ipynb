{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir, getcwd\n",
    "from glob import glob\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = getcwd()\n",
    "chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = glob('reuters21578/*.sgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_list = [\"money\", \"fx\", \"crude\", \"grain\", \"trade\", \"interest\", \"wheat\", \n",
    "              \"ship\", \"corn\", \"oil\", \"dlr\", \"gas\", \"oilseed\", \"supply\", \"sugar\", \n",
    "              \"gnp\", \"coffee\", \"veg\", \"gold\", \"soybean\", \"bop\", \"livestock\", \"cpi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_topic_in(topic, topic_list = topic_list):\n",
    "    \"\"\"function to determine if each entry belongs to our topic list\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    :param topic: list of many topics of one article\n",
    "    :param topic_list: list of pre-defined topics\n",
    "    \n",
    "    :returns: index of first element in the topic list that belongs to topic_list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ans = list(set(topic).intersection(topic_list))\n",
    "    except:\n",
    "        ans = \"\"\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanbody(text):\n",
    "    \"\"\"function to clean text by removing punctuations, and numbers\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    :param text: a string\n",
    "    \n",
    "    :returns: string with punctuations and numbers removed\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    text = text.replace('\\n',' ').lower().strip()\n",
    "    text = re.sub(\"[^a-z, A-Z]+\", \"\", text)\n",
    "    processed = ''.join(stemmer.stem(i) for i in text)\n",
    "    return(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing reuters21578/reut2-000.sgm\n",
      "Finished parsing reuters21578/reut2-000.sgm\n",
      "Start parsing reuters21578/reut2-001.sgm\n",
      "Finished parsing reuters21578/reut2-001.sgm\n",
      "Start parsing reuters21578/reut2-002.sgm\n",
      "Finished parsing reuters21578/reut2-002.sgm\n",
      "Start parsing reuters21578/reut2-003.sgm\n",
      "Finished parsing reuters21578/reut2-003.sgm\n",
      "Start parsing reuters21578/reut2-004.sgm\n",
      "Finished parsing reuters21578/reut2-004.sgm\n",
      "Start parsing reuters21578/reut2-005.sgm\n",
      "Finished parsing reuters21578/reut2-005.sgm\n",
      "Start parsing reuters21578/reut2-006.sgm\n",
      "Finished parsing reuters21578/reut2-006.sgm\n",
      "Start parsing reuters21578/reut2-007.sgm\n",
      "Finished parsing reuters21578/reut2-007.sgm\n",
      "Start parsing reuters21578/reut2-008.sgm\n",
      "Finished parsing reuters21578/reut2-008.sgm\n",
      "Start parsing reuters21578/reut2-009.sgm\n",
      "Finished parsing reuters21578/reut2-009.sgm\n",
      "Start parsing reuters21578/reut2-010.sgm\n",
      "Finished parsing reuters21578/reut2-010.sgm\n",
      "Start parsing reuters21578/reut2-011.sgm\n",
      "Finished parsing reuters21578/reut2-011.sgm\n",
      "Start parsing reuters21578/reut2-012.sgm\n",
      "Finished parsing reuters21578/reut2-012.sgm\n",
      "Start parsing reuters21578/reut2-013.sgm\n",
      "Finished parsing reuters21578/reut2-013.sgm\n",
      "Start parsing reuters21578/reut2-014.sgm\n",
      "Finished parsing reuters21578/reut2-014.sgm\n",
      "Start parsing reuters21578/reut2-015.sgm\n",
      "Finished parsing reuters21578/reut2-015.sgm\n",
      "Start parsing reuters21578/reut2-016.sgm\n",
      "Finished parsing reuters21578/reut2-016.sgm\n",
      "Start parsing reuters21578/reut2-017.sgm\n",
      "Finished parsing reuters21578/reut2-017.sgm\n",
      "Start parsing reuters21578/reut2-018.sgm\n",
      "Finished parsing reuters21578/reut2-018.sgm\n",
      "Start parsing reuters21578/reut2-019.sgm\n",
      "Finished parsing reuters21578/reut2-019.sgm\n",
      "Start parsing reuters21578/reut2-020.sgm\n",
      "Finished parsing reuters21578/reut2-020.sgm\n",
      "Start parsing reuters21578/reut2-021.sgm\n",
      "Finished parsing reuters21578/reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "doi_list = list()\n",
    "for filename in f_list:\n",
    "    print('Start parsing {0}'.format(filename))\n",
    "    file = open(filename, 'rb')\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    file.close()\n",
    "    for topic_raw in soup.find_all('topics'):\n",
    "        topic = topic_raw.get_text().split('-')\n",
    "        topic = if_topic_in(topic)\n",
    "        if len(topic) != 0:\n",
    "            body = topic_raw.find_next('body').get_text()\n",
    "            for t in topic:\n",
    "                tb_tup = (t, body)\n",
    "                doi_list.append(tb_tup)\n",
    "    print('Finished parsing {0}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total number of 3625 items were retrieved. Articles with multiple classes are recorded multiple times.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veg</td>\n",
       "      <td>argentine grain board figures show crop regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>money</td>\n",
       "      <td>sens alan cranston dcal and daniel evans rwash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>supply</td>\n",
       "      <td>sens alan cranston dcal and daniel evans rwash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>international coffee organization, ico, produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sugar</td>\n",
       "      <td>sugar imports subject to the us sugar import q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic                                               body\n",
       "0     veg  argentine grain board figures show crop regist...\n",
       "1   money  sens alan cranston dcal and daniel evans rwash...\n",
       "2  supply  sens alan cranston dcal and daniel evans rwash...\n",
       "3  coffee  international coffee organization, ico, produc...\n",
       "4   sugar  sugar imports subject to the us sugar import q..."
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(doi_list)\n",
    "data.columns = (['topic', 'body'])\n",
    "data['body'] = data['body'].apply(cleanbody)\n",
    "print('A total number of {0} items were retrieved. Articles with multiple classes are recorded multiple times.'.format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('training_test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0:10].to_csv('top10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn TFIDF processing time: 0.53331 s\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'freq_term'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'freq_term'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-3c1c37e16659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mskl_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"sklearn TFIDF processing time: {0:.5f} s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfreq_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'freq_term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#X = skl_vectorizer.transform(data['body'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'freq_term'"
     ]
    }
   ],
   "source": [
    "body_list = list(data['body'])\n",
    "start = time.clock()\n",
    "skl_vectorizer = TfidfVectorizer().fit_transform(data['body'])\n",
    "print (\"sklearn TFIDF processing time: {0:.5f} s\".format(time.clock() - start))\n",
    "freq_term_matrix = TfidfVectorizer.transform(, raw_documents=data['body'])\n",
    "#X = skl_vectorizer.transform(data['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import HashingTF, IDF, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark TFIDF processing time: 0.00971 s\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"NewsClassification\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"training_test_data.txt\",header=True,inferSchema=True)\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#This function removes stopwords (e.g. 'the', 'a') from the text\n",
    "def stop_stem(tokens):\n",
    "    tokens = tokens.split()\n",
    "    stemmed = [word for word in tokens if word not in stopwords_set]\n",
    "    return stemmed\n",
    "\n",
    "stop_stem_udf = udf(stop_stem, ArrayType(StringType()))\n",
    "df = df.withColumn(\"tokenized\", stop_stem_udf(\"body\"))\n",
    "\n",
    "#following section transforms the text using TFIDF\n",
    "start = time.clock()\n",
    "hashingTF = HashingTF(inputCol=\"tokenized\", outputCol=\"term_freq\")\n",
    "df = hashingTF.transform(df)\n",
    "idf = IDF(inputCol=\"term_freq\", outputCol=\"tfidf\", minDocFreq=5)\n",
    "idfModel = idf.fit(df)\n",
    "df = idfModel.transform(df)\n",
    "print (\"pyspark TFIDF processing time: {0:.5f} s\".format(time.clock() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the OneHotEncoder to convert the topics into discrete integers\n",
    "stringIndexer = StringIndexer(inputCol=\"topic\", outputCol=\"topicIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = dict()\n",
    "train_test_val_split_params = {'50/40/10': [0.5, 0.4, 0.1],\n",
    "                               '60/30/10': [0.6, 0.3, 0.1], \n",
    "                               '70/20/10': [0.7, 0.2, 0.1]}\n",
    "\n",
    "for split_param in train_test_val_split_params.keys():\n",
    "    for seed in np.arange(10):\n",
    "        train,test,val = indexed.select(\"tfidf\",\"topicIndex\").randomSplit(train_test_val_split_params[split_param],seed=seed)\n",
    "\n",
    "        #Naive bayes\n",
    "        nb = NaiveBayes(featuresCol=\"tfidf\", labelCol=\"topicIndex\", predictionCol=\"NB_pred\",\n",
    "                        probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "        nbModel = nb.fit(train)\n",
    "        val = nbModel.transform(val)\n",
    "        total = val.count()\n",
    "        correct = val.where(test['topicIndex'] == val['NB_pred']).count()\n",
    "        accuracy = correct/total\n",
    "        val_dict[(split_param, seed)] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('50/40/10', 0): 0.4376731301939058,\n",
       " ('50/40/10', 1): 0.506631299734748,\n",
       " ('50/40/10', 2): 0.5028248587570622,\n",
       " ('50/40/10', 3): 0.4585492227979275,\n",
       " ('50/40/10', 4): 0.4869109947643979,\n",
       " ('50/40/10', 5): 0.5014005602240896,\n",
       " ('50/40/10', 6): 0.4883720930232558,\n",
       " ('50/40/10', 7): 0.42857142857142855,\n",
       " ('50/40/10', 8): 0.44972067039106145,\n",
       " ('50/40/10', 9): 0.5068119891008175,\n",
       " ('60/30/10', 0): 0.45706371191135736,\n",
       " ('60/30/10', 1): 0.4986737400530504,\n",
       " ('60/30/10', 2): 0.5028248587570622,\n",
       " ('60/30/10', 3): 0.4637305699481865,\n",
       " ('60/30/10', 4): 0.5026178010471204,\n",
       " ('60/30/10', 5): 0.5070028011204482,\n",
       " ('60/30/10', 6): 0.4935400516795866,\n",
       " ('60/30/10', 7): 0.42587601078167114,\n",
       " ('60/30/10', 8): 0.46368715083798884,\n",
       " ('60/30/10', 9): 0.5040871934604905,\n",
       " ('70/20/10', 0): 0.4709141274238227,\n",
       " ('70/20/10', 1): 0.5013262599469496,\n",
       " ('70/20/10', 2): 0.5084745762711864,\n",
       " ('70/20/10', 3): 0.4740932642487047,\n",
       " ('70/20/10', 4): 0.49476439790575916,\n",
       " ('70/20/10', 5): 0.5070028011204482,\n",
       " ('70/20/10', 6): 0.4909560723514212,\n",
       " ('70/20/10', 7): 0.4474393530997305,\n",
       " ('70/20/10', 8): 0.46368715083798884,\n",
       " ('70/20/10', 9): 0.49318801089918257}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('70/20/10', 2)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(val_dict, key = val_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tfidf: vector, topicIndex: double]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-c3e07125ee1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "val_dict[([1],2)]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.4, 0.1]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.5, 0.4, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
